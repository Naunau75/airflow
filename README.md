# üöÄ Data Pipeline: Airflow, dbt & BigQuery

Ce projet impl√©mente un pipeline de donn√©es complet (ETL/ELT) orchestr√© par Apache Airflow, int√©grant l'ingestion de donn√©es, la transformation via dbt, et le feature engineering avanc√© avec Python/Pandas sur Google Cloud Platform (BigQuery).

## üìã Architecture du Pipeline

Le flux de donn√©es se d√©compose en 3 √©tapes principales, orchestr√©es s√©quentiellement :

1.  **Extraction (EL)** :
    *   Script : `scripts/extract.py`
    *   Action : R√©cup√®re les donn√©es utilisateurs depuis une API (JSONPlaceholder).
    *   Destination : Charge les donn√©es brutes dans BigQuery (`raw_data.users_raw`).

2.  **Transformation (T - dbt)** :
    *   Outil : dbt (data build tool)
    *   Action : Nettoyage, typage et standardisation des donn√©es.
    *   Mod√®le : `stg_users` (staging).
    *   Sortie : Table propre dans BigQuery (`dbt_staging.stg_users`).

3.  **Engineering (T - Python)** :
    *   Script : `scripts/transform.py`
    *   Action : Logique m√©tier complexe (ex: scoring utilisateurs) difficile √† impl√©menter en SQL pur.
    *   Sortie : Table analytique finale (`analytics.users_scored`).

---

## üõ† Pr√©requis

*   **Python** (>= 3.12)
*   **uv** (Gestionnaire de paquets rapide)
*   **Compte Google Cloud Platform (GCP)** avec BigQuery activ√©.
*   **Service Account GCP** avec les droits BigQuery (Admin ou Data Editor).

## ‚öôÔ∏è Installation

1.  **Cloner le d√©p√¥t** :
    ```bash
    git clone <votre-repo-url>
    cd airflow-projet
    ```

2.  **Installer les d√©pendances** avec `uv` :
    ```bash
    uv sync
    ```

3.  **Configuration dbt** :
    Assurez-vous que votre fichier `profiles.yml` est correctement configur√© pour pointer vers votre projet GCP.

## üöÄ Utilisation

### Configuration de l'environnement

Cr√©ez un fichier `.env` ou exportez la variable d'environnement n√©cessaire :

```bash
export GCP_PROJECT_ID="votre-projet-gcp-id"
```

Pour l'authentification locale, utilisez le SDK Google Cloud :
```bash
gcloud auth application-default login
```

### Ex√©cution Manuelle (Pas √† pas)

Vous pouvez tester chaque brique individuellement via `uv` :

1.  **Extraction** :
    ```bash
    uv run python ../scripts/extract.py
    ```

2.  **Transformation dbt** :
    ```bash
    cd ../dbt_project
    uv run dbt run
    ```

3.  **Transformation Python** :
    ```bash
    uv run python ../scripts/transform.py
    ```

### Orchestration avec Airflow

Le DAG est d√©fini dans `dags/pipeline_dag.py`.
*   Assurez-vous qu'Airflow est configur√© pour scanner le dossier `dags`.
*   Le DAG `my_complete_pipeline` s'ex√©cutera quotidiennement.

---

## ‚úÖ CI/CD

Le projet inclut un workflow GitHub Actions (`.github/workflows/ci.yml`) qui :
*   Installe les d√©pendances avec `uv`.
*   V√©rifie la qualit√© du code (Linting).
*   Teste la connexion dbt (`dbt debug`) √† chaque Push/PR sur la branche `main`.

## üìÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ airflow_projet/      # Configuration Python & CI
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml   # D√©pendances du projet
‚îÇ   ‚îú‚îÄ‚îÄ .github/         # Workflows CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ README.md        # Documentation
‚îú‚îÄ‚îÄ dags/                # DAGs Airflow
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_dag.py  # D√©finition du pipeline
‚îú‚îÄ‚îÄ dbt_project/         # Projet dbt
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Mod√®les SQL
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml     # Configuration connexion BQ
‚îî‚îÄ‚îÄ scripts/             # Scripts Python ETL
    ‚îú‚îÄ‚îÄ extract.py       # Ingestion API -> BQ
    ‚îî‚îÄ‚îÄ transform.py     # Logique m√©tier Python
```
