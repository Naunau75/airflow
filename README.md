# ğŸš€ Data Pipeline: Airflow, dbt & BigQuery (Kubernetes)

Ce projet implÃ©mente un pipeline de donnÃ©es complet (ETL/ELT) orchestrÃ© par Apache Airflow, s'exÃ©cutant sur **Kubernetes**. Il intÃ¨gre l'ingestion de donnÃ©es, la transformation via dbt, et le feature engineering avancÃ© avec Python/Pandas sur Google Cloud Platform.

## ğŸ“‹ Architecture du Pipeline

L'architecture repose sur la contenearisation de chaque tÃ¢che. Le DAG Airflow orchestre des **Pods Kubernetes** Ã©phÃ©mÃ¨res :

1.  **Extraction (EL)** :
    *   **Pod** : `extract-job`
    *   **Action** : RÃ©cupÃ¨re les donnÃ©es API et charge les brutes dans BigQuery (`raw_data.users_raw`).

2.  **Transformation (T - dbt)** :
    *   **Pod** : `dbt-job`
    *   **Action** : ExÃ©cute `dbt run` pour nettoyer et typer les donnÃ©es (`stg_users`).

3.  **Engineering (T - Python)** :
    *   **Pod** : `transform-job`
    *   **Action** : Logique mÃ©tier complexe en Python (`analytics.users_scored`).

---

## ğŸ›  PrÃ©requis

*   **Cluster Kubernetes** (GKE ou local via Minikube/Docker Desktop).
*   **Airflow** dÃ©ployÃ© sur Kubernetes (ou configurÃ© avec accÃ¨s au cluster).
*   **Artifact Registry** (GCP) pour stocker l'image Docker.
*   **Service Account GCP** avec les droits BigQuery.

## âš™ï¸ Installation & DÃ©ploiement

Le dÃ©ploiement repose sur une image Docker unique contenant tous les scripts et dÃ©pendances.

1.  **Build de l'image Docker** :
    ```bash
    # En local (pour test)
    docker build -t ma-pipeline-image:latest .
    ```

2.  **Configuration du DAG** :
    Modifiez `dags/pipeline_dag.py` pour pointer vers votre image Docker sur Artifact Registry :
    ```python
    image="europe-west1-docker.pkg.dev/mon-projet/mon-repo/mon-image:latest"
    ```

## ğŸš€ Utilisation

### Orchestration avec Airflow (Production)

Le DAG `my_complete_pipeline` utilise `KubernetesPodOperator`.
*   Chaque tÃ¢che dÃ©marre un conteneur isolÃ©.
*   Les logs sont remontÃ©s dans l'interface Airflow.
*   Les ressources (CPU/RAM) sont libÃ©rÃ©es aprÃ¨s chaque tÃ¢che.

### ExÃ©cution Manuelle (DÃ©veloppement)

Vous pouvez toujours exÃ©cuter les scripts manuellement en local via `uv` pour le debug :
```bash
uv run python scripts/extract.py
# etc...
```

---

## âœ… CI/CD (GitHub Actions)

Le workflow `.github/workflows/ci.yml` automatise la livraison continue :
1.  Authentification Ã  GCP (via Workload Identity).
2.  Configuration de Docker.
3.  **Build** de l'image Docker.
4.  **Push** de l'image vers Google Artifact Registry Ã  chaque merge sur `main`.

## ğŸ“‚ Structure du Projet

```
.
â”œâ”€â”€ Dockerfile           # DÃ©finition de l'image conteneur
â”œâ”€â”€ airflow_projet/      # MÃ©ta-donnÃ©es Python
â”‚   â”œâ”€â”€ pyproject.toml   # DÃ©pendances (gÃ©rÃ©es par uv)
â”‚   â”œâ”€â”€ .github/         # Workflow Build & Push
â”‚   â””â”€â”€ README.md        # Documentation
â”œâ”€â”€ dags/                # DAGs Airflow (KubernetesPodOperator)
â”œâ”€â”€ dbt_project/         # Projet dbt
â””â”€â”€ scripts/             # Scripts Python ETL
```
